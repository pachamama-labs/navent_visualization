{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_postulaciones(size=None):\n",
    "    postulaciones = pd.read_csv('data/FiubaHasta15Abril/fiuba_4_postulaciones.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idaviso'], keep='last')\n",
    "    columns_rename = {'idaviso': 'id_aviso', 'idpostulante': 'id_postulante', 'fechapostulacion': 'fecha_postulacion'}\n",
    "    postulaciones = postulaciones.rename(columns=columns_rename)\n",
    "    postulaciones['fecha_postulacion']=pd.to_datetime(postulaciones['fecha_postulacion'])\n",
    "    return postulaciones\n",
    "\n",
    "def get_vistas(size=None):\n",
    "    vistas1 = pd.read_csv('data/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas = pd.concat([vistas1,vistas2,vistas3]).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last').reset_index(drop=True)\n",
    "    if size is not None:\n",
    "        vistas = vistas.head(size)\n",
    "    columns_rename = {'idAviso': 'id_aviso', 'idpostulante': 'id_postulante', 'timestamp': 'fecha_vista'}\n",
    "    vistas = vistas.rename(columns=columns_rename)\n",
    "    vistas['fecha_vista']=pd.to_datetime(vistas['fecha_vista'])\n",
    "    \n",
    "    return vistas\n",
    "\n",
    "def get_avisos_detalle():\n",
    "    avisos1 = pd.read_csv('data/fiuba_6_avisos_detalle.csv')\n",
    "    avisos2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_6_avisos_detalle.csv')\n",
    "    avisos3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_6_avisos_detalle.csv')\n",
    "    avisos4 = pd.read_csv('data/fiuba_6_avisos_detalle_missing_nivel_laboral.csv')        \n",
    "    avisos_detalle = pd.concat([avisos1,avisos2,avisos3,avisos4]).drop_duplicates(subset=['idaviso'], keep='last').reset_index(drop=True)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'idaviso': 'id_aviso'}\n",
    "    avisos_detalle = avisos_detalle.rename(columns=columns_rename)\n",
    "    to_nivel_laboral_nro = {'Senior / Semi-Senior' : 2, 'Junior':1, 'Otro':0,\n",
    "       'Jefe / Supervisor / Responsable':3,\n",
    "       'Gerencia / Alta Gerencia / Dirección':4}\n",
    "    to_tipo_trabajo_nro={'Full-time':0, 'Part-time':1, 'Teletrabajo':2, 'Por Horas':3, 'Pasantia':4,\n",
    "       'Temporario':5, 'Por Contrato':6, 'Fines de Semana':7, 'Primer empleo':8,\n",
    "       'Voluntario':9}\n",
    "    to_nombre_area_numero = pd.Series(avisos_detalle['nombre_area'].unique()).to_dict()\n",
    "    to_nombre_area_numero  = {v: k for k, v in to_nombre_area_numero.items()}\n",
    "    avisos_detalle['nivel_laboral_nro']= avisos_detalle['nivel_laboral'].map(to_nivel_laboral_nro)\n",
    "    avisos_detalle['tipo_de_trabajo_nro'] = avisos_detalle['tipo_de_trabajo'].map(to_tipo_trabajo_nro)\n",
    "    avisos_detalle['nombre_area_nro'] = avisos_detalle['nombre_area'].map(to_nombre_area_numero)\n",
    "    \n",
    "    return avisos_detalle\n",
    "\n",
    "def get_year_of_birth(postulantes_genero_edad):\n",
    "    return (pd.to_datetime\n",
    "            (postulantes_genero_edad['fechanacimiento'], errors='coerce', format='%Y-%m-%d')\n",
    "            .dt.year)\n",
    "\n",
    "def get_age(yearOfBirth):\n",
    "    return 2018 - yearOfBirth\n",
    "    \n",
    "def get_age_range(yearOfBirth):\n",
    "    age = get_age(yearOfBirth)\n",
    "    if(age<25): return 'Entre 18 y 24'\n",
    "    if(age<30): return 'Entre 25 y 30'\n",
    "    if(age<35): return 'Entre 30 y 35'\n",
    "    if(age<40): return 'Entre 35 y 40'\n",
    "    if(age<45): return 'Entre 40 y 45'\n",
    "    if(age<50): return 'Entre 45 y 50'\n",
    "    return 'Mayor de 50'\n",
    "\n",
    "def get_order_for_age_range():\n",
    "    return ['Entre 18 y 24', 'Entre 25 y 30','Entre 30 y 35','Entre 35 y 40','Entre 40 y 45','Entre 45 y 50', 'Mayor de 50']\n",
    "\n",
    "def get_postulantes_nivel_educativo_para(path):\n",
    "    postulantes_nivel_educativo = pd.read_csv(path)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'nombre': 'formacion_postulante', 'estado': 'estado_formacion_postulante'}\n",
    "    postulantes_nivel_educativo=postulantes_nivel_educativo.rename(columns=columns_rename)\n",
    "    formacion_to_number={'Secundario' : 10, 'Otro': 20, 'Terciario/Técnico' : 30, 'Universitario' : 40, 'Posgrado' : 50,\n",
    "    'Master' : 50, 'Doctorado' : 50}\n",
    "    postulantes_nivel_educativo['formacion_postulante_numero']=postulantes_nivel_educativo['formacion_postulante'].map(formacion_to_number);\n",
    "    estado_to_number = {'En Curso': 4, 'Abandonado': 0, 'Graduado': 8}\n",
    "    postulantes_nivel_educativo['estado_formacion_postulante_numero']=postulantes_nivel_educativo['estado_formacion_postulante'].map(estado_to_number)\n",
    "    postulantes_nivel_educativo['nivel_educativo_postulante_numero'] = postulantes_nivel_educativo['formacion_postulante_numero'] + postulantes_nivel_educativo['estado_formacion_postulante_numero']\n",
    "    postulantes_nivel_educativo['nivel_educativo_postulante_texto'] = postulantes_nivel_educativo['formacion_postulante'] + ' - ' + postulantes_nivel_educativo['estado_formacion_postulante']\n",
    "    relevant_columns = ['id_postulante','nivel_educativo_postulante_texto', 'nivel_educativo_postulante_numero']\n",
    "    postulantes_nivel_educativo = postulantes_nivel_educativo[relevant_columns]\n",
    "    grouped=postulantes_nivel_educativo.groupby(['id_postulante']).agg({'nivel_educativo_postulante_numero':['max']}) \n",
    "    df=grouped.reset_index()\n",
    "    df.columns = ['id_postulante', 'maximo_nivel_educativo_postulante']\n",
    "    return df\n",
    "\n",
    "def get_postulantes_nivel_educativo():\n",
    "    postulantes1 = get_postulantes_nivel_educativo_para('data/fiuba_1_postulantes_educacion.csv')\n",
    "    postulantes2 = get_postulantes_nivel_educativo_para('data/FiubaDesde15Abril/fiuba_1_postulantes_educacion.csv')\n",
    "    postulantes3 = get_postulantes_nivel_educativo_para('data/FiubaHasta15Abril/fiuba_1_postulantes_educacion.csv')\n",
    "    return pd.concat([postulantes1,postulantes2,postulantes3]).drop_duplicates(subset=['id_postulante'], keep='last').reset_index(drop=True)\n",
    "\n",
    "def get_postulantes_genero_edad():\n",
    "    postulantes1 = pd.read_csv('data/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes_genero_edad = pd.concat([postulantes1,postulantes2,postulantes3]).drop_duplicates(subset=['idpostulante'], keep='last').reset_index(drop=True)\n",
    "    postulantes_genero_edad['año_nacimiento_postulante']=get_year_of_birth(postulantes_genero_edad)\n",
    "    postulantes_genero_edad['edad_postulante']=postulantes_genero_edad['año_nacimiento_postulante'].map(get_age, na_action=None)\n",
    "    postulantes_genero_edad['rango_edad_postulante']=postulantes_genero_edad['año_nacimiento_postulante'].map(get_age_range, na_action=None)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'fechanacimiento': 'fecha_nacimiento_postulante', 'sexo': 'genero_postulante'}\n",
    "    postulantes_genero_edad = postulantes_genero_edad.rename(columns=columns_rename)\n",
    "    postulantes_genero_edad = postulantes_genero_edad[['id_postulante', 'genero_postulante', 'fecha_nacimiento_postulante', 'edad_postulante', 'rango_edad_postulante']]\n",
    "    postulantes_genero_edad['genero_postulante_nro'] = postulantes_genero_edad['genero_postulante'].map({'FEM': 0, 'MASC': 1, 'NO_DECLARA': 2})\n",
    "    return postulantes_genero_edad\n",
    "\n",
    "def get_postulantes_limpios():\n",
    "    postulantes = pd.merge(get_postulantes_genero_edad(), get_postulantes_nivel_educativo(), on='id_postulante', how='outer')\n",
    "    order_for_columns = ['id_postulante','edad_postulante', 'genero_postulante', 'genero_postulante_nro', 'maximo_nivel_educativo_postulante']\n",
    "    return postulantes[order_for_columns]\n",
    "\n",
    "def get_detalle_postulaciones(size=None):\n",
    "    postulaciones = get_postulaciones(size)\n",
    "    avisos = get_avisos_detalle()\n",
    "    postulantes = get_postulantes_limpios()\n",
    "    detalle_postulaciones = pd.merge(postulantes, postulaciones, on='id_postulante', how='inner') \n",
    "    detalle_postulaciones = pd.merge(detalle_postulaciones, avisos, on='id_aviso', how='inner')\n",
    "    \n",
    "    return detalle_postulaciones\n",
    "\n",
    "def get_detalle_vistas(size=None):\n",
    "    vistas = get_vistas(size)\n",
    "    avisos = get_avisos_detalle()\n",
    "    postulantes = get_postulantes_limpios()\n",
    "    detalle_vistas = pd.merge(postulantes, vistas, on='id_postulante', how='inner') \n",
    "    detalle_vistas = pd.merge(detalle_vistas, avisos, on='id_aviso', how='inner')\n",
    "    return detalle_vistas\n",
    "\n",
    "def x_entrenamiento():\n",
    "    return ['edad_postulante', 'genero_postulante_nro', 'maximo_nivel_educativo_postulante', 'nivel_laboral_nro', 'tipo_de_trabajo_nro', 'nombre_area_nro']\n",
    "\n",
    "def x_categoricas():\n",
    "    return ['genero_postulante', 'maximo_nivel_educativo_postulante', 'nivel_laboral', 'tipo_de_trabajo', 'nombre_area']\n",
    "\n",
    "def x_humanas():\n",
    "    return ['id_postulante', 'id_aviso', 'titulo', 'descripcion', 'denominacion_empresa'] + x_categoricas\n",
    "\n",
    "def y_entrenamiento():\n",
    "    return ['sepostulo']\n",
    "\n",
    "def columnas_relevantes_test_data():\n",
    "    return ['id','id_aviso','id_postulante'] + x_entrenamiento()\n",
    "\n",
    "def get_test_data():\n",
    "    tests = pd.read_csv('data/Test/test_final_100k.csv')\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'idaviso': 'id_aviso'}\n",
    "    tests = tests.rename(columns=columns_rename)\n",
    "    tests = pd.merge(tests, get_avisos_detalle(), on='id_aviso', how='left')\n",
    "    tests = pd.merge(tests, get_postulantes_limpios(), on='id_postulante', how='left')\n",
    "    return tests[columnas_relevantes_test_data()]\n",
    "\n",
    "def get_default_null_values(df):\n",
    "    return {'edad_postulante':int(df['edad_postulante'].mean()), 'genero_postulante_nro':int(0), 'maximo_nivel_educativo_postulante':int(df['maximo_nivel_educativo_postulante'].mean()), 'nivel_laboral_nro':int(0), 'tipo_de_trabajo_nro':int(0), 'nombre_area_nro':int(0)}\n",
    "\n",
    "\n",
    "def get_test_data_clean():\n",
    "    tests = get_test_data()\n",
    "    tests = tests.fillna(value=get_default_null_values(tests))\n",
    "    return tests\n",
    "\n",
    "def get_datos_entrenamiento(size=None):\n",
    "    postulaciones_aplicadas = get_detalle_postulaciones(size)\n",
    "    columnas_relevantes = ['id_postulante', 'id_aviso', 'titulo', 'descripcion', 'denominacion_empresa'] + x_entrenamiento() + y_entrenamiento()\n",
    "    postulaciones_aplicadas['sepostulo'] = True\n",
    "    postulaciones_no_aplicadas = get_detalle_vistas(size)\n",
    "    postulaciones_no_aplicadas['sepostulo'] = False\n",
    "\n",
    "    return postulaciones_aplicadas.append(postulaciones_no_aplicadas).drop_duplicates(subset=['id_aviso', 'id_postulante'], keep='first')[columnas_relevantes].dropna()\n",
    "\n",
    "def get_predictor(set_entrenamiento):\n",
    "    # Prepare feature and dependent variable along with categorical encoding\n",
    "    X=pd.get_dummies(set_entrenamiento.loc[:, x_entrenamiento()])\n",
    "    y=set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = svm.SVC()\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "def get_random_forest_predictor(set_entrenamiento):\n",
    "    X = set_entrenamiento.loc[:, x_entrenamiento()]\n",
    "    Y = set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = RandomForestClassifier(n_estimators=15)\n",
    "    return clf.fit(X, Y)\n",
    "\n",
    "def get_sgd_classifier(set_entrenamiento):\n",
    "    X = set_entrenamiento.loc[:, x_entrenamiento()]\n",
    "    Y = set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = linear_model.SGDClassifier()\n",
    "    return clf.fit(X, Y)\n",
    "\n",
    "def get_general_predictor():\n",
    "    return get_random_forest_predictor(get_datos_entrenamiento(10000))\n",
    "\n",
    "def get_postulantes_a_personalizar():\n",
    "    tests = pd.read_csv('data/Test/test_final_100k.csv')\n",
    "    return set(tests['idpostulante'])\n",
    "\n",
    "def get_personalized_predictor(cantidad_mininima_observaciones):\n",
    "    datos_entrenamiento = get_datos_entrenamiento()\n",
    "    postulantes = get_postulantes_a_personalizar()\n",
    "    return get_random_forest_predictor(get_datos_entrenamiento(10000))\n",
    "\n",
    "#El ensamble\n",
    "def get_predictor_machine_avisos(cantidad_mininima_observaciones):\n",
    "    machine = { generalPredictor : get_general_predictor(), personalizedPredictor : get_personalized_predictor(cantidad_mininima_observaciones)} \n",
    "    return machine\n",
    "\n",
    "def obtener_predicciones(predictor, set_test):\n",
    "    return predictor.predict(set_test[x_entrenamiento()])\n",
    "\n",
    "def get_error_prediccion(dataset, get_predictor_to_test):\n",
    "    train=dataset.sample(frac=0.8,random_state=200)\n",
    "    test=dataset.drop(train.index)\n",
    "    predictor = get_predictor_to_test(train)\n",
    "    resultados = obtener_predicciones(predictor, test)\n",
    "    test['obtuvimos'] = resultados;\n",
    "    results = test[['sepostulo','obtuvimos']]\n",
    "    acertadas = results[results['sepostulo']==results['obtuvimos']]\n",
    "    return 1 - len(acertadas) /len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23924399055379175"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_error_prediccion(get_datos_entrenamiento(1000000), get_random_forest_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12643257809827602"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_error_prediccion(get_datos_entrenamiento(), get_random_forest_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8727"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acertadas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11784"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def obtener_predicciones(predictor, set_test):\n",
    "    return predictor.predict(set_test[x_entrenamiento()]).astype(int)\n",
    "\n",
    "def guardar_resultados(fileName, predictor, set_test):\n",
    "    result = obtener_predicciones(predictor, set_test)\n",
    "    set_test['sepostulo'] = result\n",
    "    set_test[['id','sepostulo']].set_index('id').to_csv(fileName)\n",
    "    return\n",
    "\n",
    "guardar_resultados('SGDClass.csv', get_sgd_classifier(get_datos_entrenamiento(100)), get_test_data_clean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "personalizados = 0\n",
    "totales = len(postulantes)\n",
    "cantidad_mininima_observaciones = 15\n",
    "for postulante in postulantes:\n",
    "    observaciones_postulante = datos_entrenamiento[datos_entrenamiento['id_postulante']==postulante]\n",
    "    if(len(observaciones_postulante)>=cantidad_mininima_observaciones):\n",
    "        personalizados = personalizados + 1\n",
    "print(personalizados)\n",
    "print(personalizados / totales)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
