{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de Avisos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaultHiperparametrosClusteringAvisos = { 'cantidadFeaturesTitulo' : 100, 'cantidadClusteresAvisos' : 20, 'featureListParaClustering': ['tipo_de_trabajo_nro', 'nivel_laboral_nro', 'nombre_area_nro', 'zona_nro'], 'randomState': 177 }\n",
    "\n",
    "def crear_hiperparametro_avisos(cantidadClusteresAvisos = 20, cantidadFeaturesTitulo = 100, featureListParaClustering = [], randomState = 177):\n",
    "    hiperparametros = { 'cantidadFeaturesTitulo' : cantidadFeaturesTitulo, 'cantidadClusteresAvisos' : cantidadClusteresAvisos, 'featureListParaClustering': featureListParaClustering, 'randomState': randomState}\n",
    "    return hiperparametros\n",
    "\n",
    "def agregar_clusters(avisos_detalle, hiperparametrosAvisos = None):\n",
    "    if(hiperparametrosAvisos is None):\n",
    "        hiperparametrosAvisos = defaultHiperparametrosClusteringAvisos\n",
    "    x_avisos = hiperparametrosAvisos['featureListParaClustering']\n",
    "    if(hiperparametrosAvisos['cantidadFeaturesTitulo']>0):\n",
    "        x_avisos= x_avisos + list(range(0, hiperparametrosAvisos['cantidadFeaturesTitulo']))\n",
    "        h = FeatureHasher(n_features = hiperparametrosAvisos['cantidadFeaturesTitulo'], non_negative=True, input_type='string', dtype='int')\n",
    "        x = h.transform(avisos_detalle['titulo'])\n",
    "        avisos_detalle['titulo'] = list(x.toarray())\n",
    "        titulos_como_lista = avisos_detalle.titulo.apply(pd.Series)\n",
    "        avisos_detalle = pd.merge(avisos_detalle, titulos_como_lista, left_index = True, right_index = True)\n",
    "        avisos_detalle = avisos_detalle.drop(['titulo'], axis=1)\n",
    "    if(hiperparametrosAvisos['cantidadClusteresAvisos']>0):\n",
    "        y_pred = KMeans(n_clusters=hiperparametrosAvisos['cantidadClusteresAvisos'], random_state=hiperparametrosAvisos['randomState']).fit_predict(avisos_detalle[x_avisos])\n",
    "        avisos_detalle['cluster_aviso'] = y_pred\n",
    "    return avisos_detalle\n",
    "\n",
    "def columnas_para_leer_avisos():\n",
    "    return ['descripcion', 'nombre_zona', 'tipo_de_trabajo', 'nivel_laboral', 'titulo_aviso','nombre_area','denominacion_empresa', 'cluster_aviso']\n",
    "\n",
    "def get_avisos_detalle(hiperparametrosAvisos = None):    \n",
    "    avisos1 = pd.read_csv('data/fiuba_6_avisos_detalle.csv')\n",
    "    avisos2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_6_avisos_detalle.csv')\n",
    "    avisos3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_6_avisos_detalle.csv')\n",
    "    avisos4 = pd.read_csv('data/fiuba_6_avisos_detalle_missing_nivel_laboral.csv')        \n",
    "    avisos_detalle = pd.concat([avisos1,avisos2,avisos3,avisos4]).drop_duplicates(subset=['idaviso'], keep='last').reset_index(drop=True)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'idaviso': 'id_aviso'}\n",
    "    avisos_detalle = avisos_detalle.rename(columns=columns_rename)\n",
    "    to_nivel_laboral_nro = {'Senior / Semi-Senior' : 2, 'Junior':1, 'Otro':0,\n",
    "       'Jefe / Supervisor / Responsable':3,\n",
    "       'Gerencia / Alta Gerencia / Dirección':4}\n",
    "    to_tipo_trabajo_nro={'Full-time':0, 'Part-time':1, 'Teletrabajo':2, 'Por Horas':3, 'Pasantia':4,\n",
    "       'Temporario':5, 'Por Contrato':6, 'Fines de Semana':7, 'Primer empleo':8,\n",
    "       'Voluntario':9}\n",
    "    to_nombre_area_numero = pd.Series(avisos_detalle['nombre_area'].unique()).to_dict()\n",
    "    to_nombre_area_numero  = {v: k for k, v in to_nombre_area_numero.items()}\n",
    "    to_nombre_zona_numero = pd.Series(avisos_detalle['nombre_zona'].unique()).to_dict()\n",
    "    to_nombre_zona_numero  = {v: k for k, v in to_nombre_zona_numero.items()}\n",
    "    avisos_detalle['nivel_laboral_nro']= avisos_detalle['nivel_laboral'].map(to_nivel_laboral_nro)\n",
    "    avisos_detalle['nivel_laboral_nro'].fillna(value =0, inplace = True)\n",
    "    avisos_detalle['tipo_de_trabajo_nro'] = avisos_detalle['tipo_de_trabajo'].map(to_tipo_trabajo_nro)\n",
    "    avisos_detalle['nombre_area_nro'] = avisos_detalle['nombre_area'].map(to_nombre_area_numero)\n",
    "    avisos_detalle['zona_nro'] = avisos_detalle['nombre_zona'].map(to_nombre_zona_numero)\n",
    "    avisos_detalle['titulo_aviso'] = avisos_detalle['titulo']    \n",
    "    \n",
    "    return agregar_clusters(avisos_detalle, hiperparametrosAvisos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacion datos entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postulaciones(size=None):\n",
    "    postulaciones = pd.read_csv('data/FiubaHasta15Abril/fiuba_4_postulaciones.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idaviso'], keep='last')\n",
    "    columns_rename = {'idaviso': 'id_aviso', 'idpostulante': 'id_postulante', 'fechapostulacion': 'fecha_postulacion'}\n",
    "    postulaciones = postulaciones.rename(columns=columns_rename)\n",
    "    postulaciones['fecha_postulacion']=pd.to_datetime(postulaciones['fecha_postulacion'])\n",
    "    return postulaciones\n",
    "\n",
    "def get_vistas(size=None):\n",
    "    vistas1 = pd.read_csv('data/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_3_vistas.csv', nrows =size).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last')\n",
    "    vistas = pd.concat([vistas1,vistas2,vistas3]).drop_duplicates(subset=['idpostulante', 'idAviso'], keep='last').reset_index(drop=True)\n",
    "    if size is not None:\n",
    "        vistas = vistas.head(size)\n",
    "    columns_rename = {'idAviso': 'id_aviso', 'idpostulante': 'id_postulante', 'timestamp': 'fecha_vista'}\n",
    "    vistas = vistas.rename(columns=columns_rename)\n",
    "    vistas['fecha_vista']=pd.to_datetime(vistas['fecha_vista'])\n",
    "    \n",
    "    return vistas\n",
    "\n",
    "def get_year_of_birth(postulantes_genero_edad):\n",
    "    return (pd.to_datetime\n",
    "            (postulantes_genero_edad['fechanacimiento'], errors='coerce', format='%Y-%m-%d')\n",
    "            .dt.year)\n",
    "\n",
    "def get_age(yearOfBirth):\n",
    "    return 2018 - yearOfBirth\n",
    "    \n",
    "def get_age_range(yearOfBirth):\n",
    "    age = get_age(yearOfBirth)\n",
    "    if(age<25): return 'Entre 18 y 24'\n",
    "    if(age<30): return 'Entre 25 y 30'\n",
    "    if(age<35): return 'Entre 30 y 35'\n",
    "    if(age<40): return 'Entre 35 y 40'\n",
    "    if(age<45): return 'Entre 40 y 45'\n",
    "    if(age<50): return 'Entre 45 y 50'\n",
    "    return 'Mayor de 50'\n",
    "\n",
    "def get_order_for_age_range():\n",
    "    return ['Entre 18 y 24', 'Entre 25 y 30','Entre 30 y 35','Entre 35 y 40','Entre 40 y 45','Entre 45 y 50', 'Mayor de 50']\n",
    "\n",
    "def get_postulantes_nivel_educativo_para(path):\n",
    "    postulantes_nivel_educativo = pd.read_csv(path)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'nombre': 'formacion_postulante', 'estado': 'estado_formacion_postulante'}\n",
    "    postulantes_nivel_educativo=postulantes_nivel_educativo.rename(columns=columns_rename)\n",
    "    formacion_to_number={'Secundario' : 10, 'Otro': 20, 'Terciario/Técnico' : 30, 'Universitario' : 40, 'Posgrado' : 50,\n",
    "    'Master' : 50, 'Doctorado' : 50}\n",
    "    postulantes_nivel_educativo['formacion_postulante_numero']=postulantes_nivel_educativo['formacion_postulante'].map(formacion_to_number);\n",
    "    estado_to_number = {'En Curso': 4, 'Abandonado': 0, 'Graduado': 8}\n",
    "    postulantes_nivel_educativo['estado_formacion_postulante_numero']=postulantes_nivel_educativo['estado_formacion_postulante'].map(estado_to_number)\n",
    "    postulantes_nivel_educativo['nivel_educativo_postulante_numero'] = postulantes_nivel_educativo['formacion_postulante_numero'] + postulantes_nivel_educativo['estado_formacion_postulante_numero']\n",
    "    postulantes_nivel_educativo['nivel_educativo_postulante_texto'] = postulantes_nivel_educativo['formacion_postulante'] + ' - ' + postulantes_nivel_educativo['estado_formacion_postulante']\n",
    "    relevant_columns = ['id_postulante','nivel_educativo_postulante_texto', 'nivel_educativo_postulante_numero']\n",
    "    postulantes_nivel_educativo = postulantes_nivel_educativo[relevant_columns]\n",
    "    grouped=postulantes_nivel_educativo.groupby(['id_postulante']).agg({'nivel_educativo_postulante_numero':['max']}) \n",
    "    df=grouped.reset_index()\n",
    "    df.columns = ['id_postulante', 'maximo_nivel_educativo_postulante']\n",
    "    return df\n",
    "\n",
    "def get_postulantes_nivel_educativo():\n",
    "    postulantes1 = get_postulantes_nivel_educativo_para('data/fiuba_1_postulantes_educacion.csv')\n",
    "    postulantes2 = get_postulantes_nivel_educativo_para('data/FiubaDesde15Abril/fiuba_1_postulantes_educacion.csv')\n",
    "    postulantes3 = get_postulantes_nivel_educativo_para('data/FiubaHasta15Abril/fiuba_1_postulantes_educacion.csv')\n",
    "    return pd.concat([postulantes1,postulantes2,postulantes3]).drop_duplicates(subset=['id_postulante'], keep='last').reset_index(drop=True)\n",
    "\n",
    "def get_postulantes_genero_edad():\n",
    "    postulantes1 = pd.read_csv('data/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes2 = pd.read_csv('data/FiubaDesde15Abril/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes3 = pd.read_csv('data/FiubaHasta15Abril/fiuba_2_postulantes_genero_y_edad.csv')\n",
    "    postulantes_genero_edad = pd.concat([postulantes1,postulantes2,postulantes3]).drop_duplicates(subset=['idpostulante'], keep='last').reset_index(drop=True)\n",
    "    postulantes_genero_edad['año_nacimiento_postulante']=get_year_of_birth(postulantes_genero_edad)\n",
    "    postulantes_genero_edad['edad_postulante']=postulantes_genero_edad['año_nacimiento_postulante'].map(get_age, na_action=None)\n",
    "    postulantes_genero_edad['rango_edad_postulante']=postulantes_genero_edad['año_nacimiento_postulante'].map(get_age_range, na_action=None)\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'fechanacimiento': 'fecha_nacimiento_postulante', 'sexo': 'genero_postulante'}\n",
    "    postulantes_genero_edad = postulantes_genero_edad.rename(columns=columns_rename)\n",
    "    postulantes_genero_edad = postulantes_genero_edad[['id_postulante', 'genero_postulante', 'fecha_nacimiento_postulante', 'edad_postulante', 'rango_edad_postulante']]\n",
    "    postulantes_genero_edad['genero_postulante'] = postulantes_genero_edad['genero_postulante'].map({'FEM': 0, 'MASC': 1, 'NO_DECLARA': 2})\n",
    "    return postulantes_genero_edad\n",
    "\n",
    "def get_postulantes_limpios():\n",
    "    postulantes = pd.merge(get_postulantes_genero_edad(), get_postulantes_nivel_educativo(), on='id_postulante', how='outer')\n",
    "    order_for_columns = ['id_postulante','edad_postulante', 'genero_postulante', 'maximo_nivel_educativo_postulante']\n",
    "    return postulantes[order_for_columns]\n",
    "\n",
    "def get_detalle_postulaciones(size, avisos, postulantes):\n",
    "    postulaciones = get_postulaciones(size)    \n",
    "    detalle_postulaciones = pd.merge(postulantes, postulaciones, on='id_postulante', how='inner') \n",
    "    detalle_postulaciones = pd.merge(detalle_postulaciones, avisos, on='id_aviso', how='inner')\n",
    "    \n",
    "    return detalle_postulaciones\n",
    "\n",
    "def get_detalle_vistas(size, avisos, postulantes):\n",
    "    vistas = get_vistas(size)\n",
    "    detalle_vistas = pd.merge(postulantes, vistas, on='id_postulante', how='inner') \n",
    "    detalle_vistas = pd.merge(detalle_vistas, avisos, on='id_aviso', how='inner')\n",
    "    return detalle_vistas\n",
    "\n",
    "def x_entrenamiento():\n",
    "    return ['edad_postulante', 'genero_postulante', 'maximo_nivel_educativo_postulante', 'nivel_laboral_nro', 'nombre_area_nro', 'cluster_aviso']\n",
    "\n",
    "def y_entrenamiento():\n",
    "    return ['sepostulo']\n",
    "\n",
    "def columnas_relevantes_test_data():\n",
    "    return ['id','id_aviso','id_postulante'] + x_entrenamiento()\n",
    "\n",
    "def get_test_data():\n",
    "    tests = pd.read_csv('data/Test/test_final_100k.csv')\n",
    "    columns_rename = {'idpostulante': 'id_postulante', 'idaviso': 'id_aviso'}\n",
    "    tests = tests.rename(columns=columns_rename)\n",
    "    tests = pd.merge(tests, get_avisos_detalle(), on='id_aviso', how='left')\n",
    "    tests = pd.merge(tests, get_postulantes_limpios(), on='id_postulante', how='left')\n",
    "    return tests[columnas_relevantes_test_data()]\n",
    "\n",
    "def get_default_null_values(df):\n",
    "    return {'edad_postulante':int(df['edad_postulante'].mean()), 'genero_postulante':int(0), 'maximo_nivel_educativo_postulante':int(df['maximo_nivel_educativo_postulante'].mean()), 'nivel_laboral': int(df['nivel_laboral'].mean()), 'tipo_de_trabajo_nro':int(0), 'nombre_area_nro':int(0)}\n",
    "\n",
    "def get_test_data_clean():\n",
    "    tests = get_test_data()\n",
    "    tests = tests.fillna(value=get_default_null_values(tests))\n",
    "    return tests\n",
    "\n",
    "def get_datos_entrenamiento(size=None, hiperparametrosClusteringAvisos = None):\n",
    "    avisos = get_avisos_detalle(hiperparametrosClusteringAvisos)\n",
    "    postulantes = get_postulantes_limpios()\n",
    "    postulaciones_aplicadas = get_detalle_postulaciones(size, avisos, postulantes)\n",
    "    columnas_relevantes = ['id_postulante', 'id_aviso', 'descripcion', 'denominacion_empresa'] + x_entrenamiento() + y_entrenamiento()\n",
    "    postulaciones_aplicadas['sepostulo'] = True\n",
    "    postulaciones_no_aplicadas = get_detalle_vistas(size, avisos, postulantes)\n",
    "    postulaciones_no_aplicadas['sepostulo'] = False\n",
    "\n",
    "    return postulaciones_aplicadas.append(postulaciones_no_aplicadas).drop_duplicates(subset=['id_aviso', 'id_postulante'], keep='first')[columnas_relevantes].dropna()\n",
    "\n",
    "def get_predictor(set_entrenamiento):\n",
    "    # Prepare feature and dependent variable along with categorical encoding\n",
    "    X=pd.get_dummies(set_entrenamiento.loc[:, x_entrenamiento()])\n",
    "    y=set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = svm.SVC()\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "def get_random_forest_predictor(set_entrenamiento):\n",
    "    X = set_entrenamiento.loc[:, x_entrenamiento()]\n",
    "    Y = set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = RandomForestClassifier(n_estimators=15)\n",
    "    return clf.fit(X, Y)\n",
    "\n",
    "def get_sgd_classifier(set_entrenamiento):\n",
    "    X = set_entrenamiento.loc[:, x_entrenamiento()]\n",
    "    Y = set_entrenamiento.loc[:, 'sepostulo']\n",
    "    clf = linear_model.SGDClassifier()\n",
    "    return clf.fit(X, Y)\n",
    "\n",
    "def get_general_predictor():\n",
    "    return get_random_forest_predictor(get_datos_entrenamiento(10000))\n",
    "\n",
    "def get_postulantes_a_personalizar():\n",
    "    tests = pd.read_csv('data/Test/test_final_100k.csv')\n",
    "    return set(tests['idpostulante'])\n",
    "\n",
    "def get_personalized_predictor(cantidad_mininima_observaciones):\n",
    "    datos_entrenamiento = get_datos_entrenamiento()\n",
    "    postulantes = get_postulantes_a_personalizar()\n",
    "    return get_random_forest_predictor(get_datos_entrenamiento(10000))\n",
    "\n",
    "#El ensamble\n",
    "def get_predictor_machine_avisos(cantidad_mininima_observaciones):\n",
    "    machine = { generalPredictor : get_general_predictor(), personalizedPredictor : get_personalized_predictor(cantidad_mininima_observaciones)} \n",
    "    return machine\n",
    "\n",
    "def obtener_predicciones(predictor, set_test):\n",
    "    return predictor.predict(set_test[x_entrenamiento()])\n",
    "\n",
    "def get_score(dataset, clasificador):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.loc[:, x_entrenamiento()], dataset.loc[:, 'sepostulo'], test_size=0.33, random_state=42)\n",
    "    clasificador = clasificador.fit(X_train, y_train)    \n",
    "    return (clasificador.score(X_test, y_test), clasificador)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([            'id_aviso',               'idpais',          'descripcion',\n",
      "                'nombre_zona',               'ciudad',            'mapacalle',\n",
      "            'tipo_de_trabajo',        'nivel_laboral',          'nombre_area',\n",
      "       'denominacion_empresa',\n",
      "       ...\n",
      "                           91,                     92,                     93,\n",
      "                           94,                     95,                     96,\n",
      "                           97,                     98,                     99,\n",
      "              'cluster_aviso'],\n",
      "      dtype='object', length=116)\n",
      "Index([                    'id_postulante',\n",
      "                         'edad_postulante',\n",
      "                       'genero_postulante',\n",
      "       'maximo_nivel_educativo_postulante',\n",
      "                                'id_aviso',\n",
      "                       'fecha_postulacion',\n",
      "                                  'idpais',\n",
      "                             'descripcion',\n",
      "                             'nombre_zona',\n",
      "                                  'ciudad',\n",
      "       ...\n",
      "                                        92,\n",
      "                                        93,\n",
      "                                        94,\n",
      "                                        95,\n",
      "                                        96,\n",
      "                                        97,\n",
      "                                        98,\n",
      "                                        99,\n",
      "                           'cluster_aviso',\n",
      "                               'sepostulo'],\n",
      "      dtype='object', length=122)\n",
      "Index([                    'id_postulante',\n",
      "                         'edad_postulante',\n",
      "                       'genero_postulante',\n",
      "       'maximo_nivel_educativo_postulante',\n",
      "                                'id_aviso',\n",
      "                             'fecha_vista',\n",
      "                                  'idpais',\n",
      "                             'descripcion',\n",
      "                             'nombre_zona',\n",
      "                                  'ciudad',\n",
      "       ...\n",
      "                                        92,\n",
      "                                        93,\n",
      "                                        94,\n",
      "                                        95,\n",
      "                                        96,\n",
      "                                        97,\n",
      "                                        98,\n",
      "                                        99,\n",
      "                           'cluster_aviso',\n",
      "                               'sepostulo'],\n",
      "      dtype='object', length=122)\n"
     ]
    }
   ],
   "source": [
    "dataset = get_datos_entrenamiento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "predictores = []\n",
    "predictores.append(get_score(dataset,linear_model.SGDClassifier()))\n",
    "predictores.append(get_score(dataset,RandomForestClassifier(n_estimators=15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.44655478013124683, SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)), (0.59044278765645675, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "print(predictores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mejor_predictor = predictores[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtener_predicciones(predictor, set_test):\n",
    "    return predictor.predict(set_test[x_entrenamiento()]).astype(int)\n",
    "\n",
    "def guardar_resultados(fileName, predictor, set_test):\n",
    "    result = obtener_predicciones(predictor, set_test)\n",
    "    set_test['sepostulo'] = result\n",
    "    set_test[['id','sepostulo']].set_index('id').to_csv(fileName)\n",
    "    return\n",
    "\n",
    "guardar_resultados('randomForestMarian.csv', mejor_predictor, get_test_data_clean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
